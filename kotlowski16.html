<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Online Isotonic Regression | COLT 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Online Isotonic Regression">

  <meta name="citation_author" content="Kot&lt;span&gt;ł&lt;/span&gt;owski, Wojciech">

  <meta name="citation_author" content="Koolen, Wouter M.">

  <meta name="citation_author" content="Malek, Alan">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="29th Annual Conference on Learning Theory">
<meta name="citation_firstpage" content="1165">
<meta name="citation_lastpage" content="1189">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v49/kotlowski16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Online Isotonic Regression</h1>

	<div id="authors">
	
		Wojciech Kot<span>ł</span>owski,
	
		Wouter M. Koolen,
	
		Alan Malek
	<br />
	</div>
	<div id="info">
		29th Annual Conference on Learning Theory,
		pp. 1165–1189, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (non-decreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by <span class="math">\(O\big(T^{1/3} \log^{2/3}(T)\big)\)</span> and present a matching <span class="math">\(\Omega(T^{1/3})\)</span> lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to <span class="math">\(O(\log T)\)</span> or even to <span class="math">\(O(1)\)</span> (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for entropic loss and absolute loss.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="kotlowski16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
