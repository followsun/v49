<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>benefits of depth in neural networks | COLT 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="benefits of depth in neural networks">

  <meta name="citation_author" content="Telgarsky, Matus">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="29th Annual Conference on Learning Theory">
<meta name="citation_firstpage" content="1517">
<meta name="citation_lastpage" content="1539">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v49/telgarsky16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>benefits of depth in neural networks</h1>

	<div id="authors">
	
		Matus Telgarsky
	<br />
	</div>
	<div id="info">
		29th Annual Conference on Learning Theory,
		pp. 1517–1539, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		For any positive integer <span class="math">\(k\)</span>, there exist neural networks with <span class="math">\(\Theta(k^3)\)</span> layers, <span class="math">\(\Theta(1)\)</span> nodes per layer, and <span class="math">\(\Theta(1)\)</span> distinct parameters which can not be approximated by networks with <span class="math">\(O(k)\)</span> layers unless they are exponentially large — they must possess <span class="math">\(\Omega(2^k)\)</span> nodes. This result is proved here for a class of nodes termed <em>semi-algebraic gates</em> which includes the common choices of ReLU, maximum, indicator, and piecewise polynomial functions, therefore establishing benefits of depth against not just standard networks with ReLU gates, but also convolutional networks with ReLU and maximization gates, sum-product networks, and boosted decision trees (in this last case with a stronger separation: <span class="math">\(\Omega(2^{k^3})\)</span> total tree nodes are required).
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="telgarsky16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
