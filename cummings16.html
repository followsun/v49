<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Adaptive Learning with Robust Generalization Guarantees | COLT 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Adaptive Learning with Robust Generalization Guarantees">

  <meta name="citation_author" content="Cummings, Rachel">

  <meta name="citation_author" content="Ligett, Katrina">

  <meta name="citation_author" content="Nissim, Kobbi">

  <meta name="citation_author" content="Roth, Aaron">

  <meta name="citation_author" content="Wu, Zhiwei Steven">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="29th Annual Conference on Learning Theory">
<meta name="citation_firstpage" content="772">
<meta name="citation_lastpage" content="814">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v49/cummings16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Adaptive Learning with Robust Generalization Guarantees</h1>

	<div id="authors">
	
		Rachel Cummings,
	
		Katrina Ligett,
	
		Kobbi Nissim,
	
		Aaron Roth,
	
		Zhiwei Steven Wu
	<br />
	</div>
	<div id="info">
		29th Annual Conference on Learning Theory,
		pp. 772–814, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		The traditional notion of <em>generalization</em>—i.e., learning a hypothesis whose empirical error is close to its true error—is surprisingly brittle. As has recently been noted [Dwork et al. 2015], even if several algorithms have this guarantee in isolation, the guarantee need not hold if the algorithms are composed adaptively. In this paper, we study three notions of generalization—increasing in strength—that are <em>robust</em> to postprocessing and amenable to adaptive composition, and examine the relationships between them. We call the weakest such notion <em>Robust Generalization</em>. A second, intermediate, notion is the stability guarantee known as <em>differential privacy</em>. The strongest guarantee we consider we call <em>Perfect Generalization</em>. We prove that every hypothesis class that is PAC learnable is also PAC learnable in a robustly generalizing fashion, with almost the same sample complexity. It was previously known that differentially private algorithms satisfy robust generalization. In this paper, we show that robust generalization is a strictly weaker concept, and that there is a learning task that can be carried out subject to robust generalization guarantees, yet cannot be carried out subject to differential privacy. We also show that perfect generalization is a strictly stronger guarantee than differential privacy, but that, nevertheless, many learning tasks can be carried out subject to the guarantees of perfect generalization.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="cummings16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
