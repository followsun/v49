<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Online Sparse Linear Regression | COLT 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Online Sparse Linear Regression">

  <meta name="citation_author" content="Foster, Dean">

  <meta name="citation_author" content="Kale, Satyen">

  <meta name="citation_author" content="Karloff, Howard">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="29th Annual Conference on Learning Theory">
<meta name="citation_firstpage" content="960">
<meta name="citation_lastpage" content="970">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v49/foster16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Online Sparse Linear Regression</h1>

	<div id="authors">
	
		Dean Foster,
	
		Satyen Kale,
	
		Howard Karloff
	<br />
	</div>
	<div id="info">
		29th Annual Conference on Learning Theory,
		pp. 960â€“970, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an <span><em>inefficient</em></span> algorithm that obtains regret bounded by <span class="math">\(\tilde{O}(\sqrt{T})\)</span> after <span class="math">\(T\)</span> prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by <span class="math">\(O(T^{1-\delta})\)</span> for any constant <span class="math">\(\delta &gt; 0\)</span> unless <span class="math">\(\textsf{NP} \subseteq \textsf{BPP}\)</span>. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="foster16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
