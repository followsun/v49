---
title: A Light Touch for Heavily Constrained SGD
abstract: Minimizing empirical risk subject to a set of constraints can be a useful
  strategy for learning restricted classes of functions, such as monotonic functions,
  submodular functions, classifiers that guarantee a certain class label for some
  subset of examples, etc. However, these restrictions may result in a very large
  number of constraints. Projected stochastic gradient descent (SGD) is often the
  default choice for large-scale optimization in machine learning, but requires a
  projection after each update. For heavily-constrained objectives, we propose an
  efficient extension of SGD that stays close to the feasible region while only applying
  constraints probabilistically at each iteration. Theoretical analysis shows a compelling
  trade-off between per-iteration work and the number of iterations needed on problems
  with a large number of constraints.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cotter16
month: 0
firstpage: 729
lastpage: 771
page: 729-771
sections: 
author:
- given: Andrew
  family: Cotter
- given: Maya
  family: Gupta
- given: Jan
  family: Pfeifer
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/cotter16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
