---
title: Reinforcement Learning of POMDPs using Spectral Methods
abstract: We propose a new reinforcement learning algorithm for partially observable
  Markov decision processes (POMDP) based on spectral decomposition methods. While
  spectral methods have been previously employed for consistent learning of (passive)
  latent variable models such as hidden Markov models, POMDPs are more challenging  since
  the learner interacts with the environment and possibly changes the future observations
  in the process. We devise a learning algorithm running through episodes, in each
  episode we employ spectral techniques to learn the POMDP parameters from a trajectory
  generated by a fixed policy. At the end of the episode, an optimization oracle returns
  the optimal memoryless planning policy which maximizes the expected reward based
  on the estimated POMDP model. We prove an order-optimal regret bound w.r.t. the
  optimal memoryless policy and efficient scaling with respect to the dimensionality
  of observation and action spaces.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: azizzadenesheli16a
month: 0
tex_title: Reinforcement Learning of POMDPs using Spectral Methods
firstpage: 193
lastpage: 256
page: 193-256
order: 193
cycles: false
author:
- given: Kamyar
  family: Azizzadenesheli
- given: Alessandro
  family: Lazaric
- given: Animashree
  family: Anandkumar
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/azizzadenesheli16a.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
