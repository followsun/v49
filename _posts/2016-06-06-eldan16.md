---
title: The Power of Depth for Feedforward Neural Networks
abstract: 'We show that there is a simple (approximately radial) function on \mathbbR^d,
  expressible by a small 3-layer feedforward neural networks, which cannot be approximated
  by any 2-layer network, to more than a certain constant accuracy, unless its width
  is exponential in the dimension. The result holds for virtually all known activation
  functions, including rectified linear units, sigmoids and thresholds, and formally
  demonstrates that depth – even if increased by 1 – can be exponentially more valuable
  than width for standard feedforward neural networks. Moreover, compared to related
  results in the context of Boolean functions, our result requires fewer assumptions,
  and the proof techniques and construction are very different. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: eldan16
month: 0
firstpage: 907
lastpage: 940
page: 907-940
sections: 
author:
- given: Ronen
  family: Eldan
- given: Ohad
  family: Shamir
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/eldan16/eldan16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
