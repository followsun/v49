---
title: Online Sparse Linear Regression
abstract: We consider the online sparse linear regression problem, which is the problem
  of sequentially making predictions observing only a limited number of features in
  each round, to minimize regret with respect to the best sparse linear regressor,
  where prediction accuracy is measured by square loss. We give an \em inefficient
  algorithm that obtains regret bounded by \tildeO(\sqrtT) after T prediction rounds.
  We complement this result by showing that no algorithm running in polynomial time
  per iteration can achieve regret bounded by O(T^1-δ) for any constant δ> 0 unless
  \textsfNP ⊆\textsfBPP. This computational hardness result resolves an open problem
  presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This
  hardness result holds even if the algorithm is allowed to access more features than
  the best sparse linear regressor up to a logarithmic factor in the dimension.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: foster16
month: 0
firstpage: 960
lastpage: 970
page: 960-970
sections: 
author:
- given: Dean
  family: Foster
- given: Satyen
  family: Kale
- given: Howard
  family: Karloff
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/foster16/foster16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
