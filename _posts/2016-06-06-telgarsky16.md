---
title: benefits of depth in neural networks
abstract: 'For any positive integer k, there exist neural networks with Θ(k^3) layers,
  Θ(1) nodes per layer, and Θ(1) distinct parameters which can not be approximated
  by networks with O(k) layers unless they are exponentially large — they must possess
  Ω(2^k) nodes. This result is proved here for a class of nodes termed \emphsemi-algebraic
  gates which includes the common choices of ReLU, maximum, indicator, and piecewise
  polynomial functions, therefore establishing benefits of depth against not just
  standard networks with ReLU gates, but also convolutional networks with ReLU and
  maximization gates, sum-product networks, and boosted decision trees (in this last
  case with a stronger separation: Ω(2^k^3) total tree nodes are required). '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: telgarsky16
month: 0
tex_title: benefits of depth in neural networks
firstpage: 1517
lastpage: 1539
page: 1517-1539
order: 1517
cycles: false
author:
- given: Matus
  family: Telgarsky
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/telgarsky16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
