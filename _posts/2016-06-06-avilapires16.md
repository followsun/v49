---
title: Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear
  Models
abstract: In this paper we study a model-based approach to calculating approximately
  optimal policies in Markovian Decision Processes. In particular, we derive novel
  bounds on the loss of using a policy derived from a factored linear model, a class
  of models which generalize numerous previous models out of those that come with
  strong computational guarantees. For the first time in the literature, we derive
  performance bounds for model-based techniques where the model inaccuracy is measured
  in weighted norms. Moreover, our bounds show a decreased sensitivity to the discount
  factor and, unlike similar bounds derived for other approaches, they are insensitive
  to measure mismatch. Similarly to previous works, our proofs are also based on contraction
  arguments, but with the main differences that we use carefully constructed norms
  building on Banach lattices, and the contraction property is only assumed for operators
  acting on “compressed” spaces, thus weakening previous assumptions, while strengthening
  previous results.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: avilapires16
month: 0
tex_title: Policy Error Bounds for Model-Based Reinforcement Learning with Factored
  Linear Models
firstpage: 121
lastpage: 151
page: 121-151
sections: 
author:
- given: Bernardo
  family: "Ávila Pires"
- given: Csaba
  family: Szepesvári
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/avilapires16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
