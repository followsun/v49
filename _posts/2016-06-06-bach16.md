---
title: Highly-Smooth Zero-th Order Online Optimization
abstract: The minimization of convex functions which are only available through partial
  and noisy information is a key methodological problem in many disciplines. In this
  paper we consider  convex optimization with noisy zero-th order information, that
  is noisy function evaluations at any desired point. We focus on problems with high
  degrees of smoothness, such as  logistic regression. We show that as opposed to
  gradient-based algorithms, high-order smoothness may be used to improve estimation
  rates, with a precise dependence  of our upper-bounds on the degree of smoothness.
  In particular, we show that for infinitely differentiable functions, we recover
  the same dependence on sample size as gradient-based algorithms, with an extra dimension-dependent
  factor. This is done for both convex and strongly-convex functions, with finite
  horizon and anytime algorithms. Finally, we also recover similar results in the
  online optimization setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bach16
month: 0
tex_title: Highly-Smooth Zero-th Order Online Optimization
firstpage: 257
lastpage: 283
page: 257-283
sections: 
author:
- given: Francis
  family: Bach
- given: Vianney
  family: Perchet
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/bach16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
