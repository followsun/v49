---
title: Online Isotonic Regression
abstract: We consider the online version of the isotonic regression problem. Given
  a set of linearly ordered points (e.g., on the real line), the learner must predict
  labels sequentially at adversarially chosen positions and is evaluated by her total
  squared loss compared against the best isotonic (non-decreasing) function in hindsight.
  We survey several standard online learning algorithms and show that none of them
  achieve the optimal regret exponent; in fact, most of them (including Online Gradient
  Descent, Follow the Leader and Exponential Weights) incur linear regret. We then
  prove that the Exponential Weights algorithm played over a covering net of isotonic
  functions has a regret bounded by O\big(T^1/3 \log^2/3(T)\big) and present a matching
  Ω(T^1/3) lower bound on regret. We provide a computationally efficient version of
  this algorithm. We also analyze the noise-free case, in which the revealed labels
  are isotonic, and show that the bound can be improved to O(\log T) or even to O(1)
  (when the labels are revealed in isotonic order). Finally, we extend the analysis
  beyond squared loss and give  bounds for entropic loss and absolute loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kotlowski16
month: 0
tex_title: Online Isotonic Regression
firstpage: 1165
lastpage: 1189
page: 1165-1189
sections: 
author:
- given: Wojciech
  family: Kotłowski
- given: Wouter M.
  family: Koolen
- given: Alan
  family: Malek
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/kotlowski16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
