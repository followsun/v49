---
title: Efficient approaches for escaping higher order saddle points  in non-convex
  optimization
abstract: 'Local search heuristics for non-convex optimizations are popular   in applied
  machine learning. However, in general it is  hard to  guarantee that such algorithms
  even  converge to a \em local minimum, due to the existence of complicated saddle
  point structures in high dimensions. Many functions have \em degenerate saddle points
  such that the first and second order derivatives cannot distinguish them with local
  optima.  In this paper we use higher order derivatives to escape these saddle points:
  we design the first efficient algorithm  guaranteed to converge to a third order
  local optimum (while existing techniques are at most second order). We also show
  that it is NP-hard to extend this further to finding fourth order local optima.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: anandkumar16
month: 0
tex_title: Efficient approaches for escaping higher order saddle points  in non-convex
  optimization
firstpage: 81
lastpage: 102
page: 81-102
order: 81
cycles: false
author:
- given: Animashree
  family: Anandkumar
- given: Rong
  family: Ge
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/anandkumar16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
