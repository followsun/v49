---
title: First-order Methods for Geodesically Convex Optimization
abstract: Geodesic convexity generalizes the notion of (vector space) convexity to
  nonlinear metric spaces. But unlike convex optimization, geodesically convex (g-convex)
  optimization is much less developed. In this paper we contribute to the understanding
  of g-convex optimization by developing iteration complexity analysis for several
  first-order algorithms on Hadamard manifolds. Specifically, we prove upper bounds
  for the global complexity of deterministic and stochastic (sub)gradient methods
  for optimizing smooth and nonsmooth g-convex functions, both with and without strong
  g-convexity. Our analysis also reveals how the manifold geometry, especially \emphsectional
  curvature, impacts convergence rates. To the best of our knowledge, our work is
  the first to provide global complexity analysis for first-order algorithms for general
  g-convex optimization.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang16b
month: 0
firstpage: 1617
lastpage: 1638
page: 1617-1638
sections: 
author:
- given: Hongyi
  family: Zhang
- given: Suvrit
  family: Sra
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/zhang16b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
