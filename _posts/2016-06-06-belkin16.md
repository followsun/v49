---
title: Basis Learning as an Algorithmic Primitive
abstract: A number of important problems in theoretical computer science and machine
  learning can be interpreted as recovering a certain basis. These include  symmetric
  matrix eigendecomposition, certain tensor decompositions, Independent Component
  Analysis (ICA), spectral clustering and Gaussian mixture learning. Each of these
  problems reduces to an instance of our general model, which we call a “Basis Encoding
  Function" (BEF). We show that learning a basis within this model can then be provably
  and efficiently achieved using a first order  iteration algorithm (gradient iteration).
  Our algorithm goes beyond tensor methods while generalizing a number of existing
  algorithms—e.g., the power method for symmetric matrices, the tensor power iteration
  for orthogonal decomposable tensors, and cumulant-based FastICA—all within a broader
  function-based dynamical systems framework. Our framework also unifies the unusual
  phenomenon observed in these domains that they can be solved using efficient non-convex
  optimization. Specifically, we describe a class of BEFs such that their local maxima
  on the unit sphere are in one-to-one correspondence with the basis elements. This
  description relies on a certain “hidden convexity" property of these functions.
  We provide a complete theoretical analysis of the gradient iteration even when the
  BEF is perturbed. We show convergence and complexity bounds polynomial in dimension
  and other relevant parameters, such as perturbation size. Our perturbation results
  can be considered as a  non-linear version of the classical Davis-Kahan theorem
  for perturbations of eigenvectors of symmetric matrices. In addition we show that   our
  algorithm exhibits fast (superlinear) convergence and relate the speed of convergence
  to the properties of the BEF. Moreover, the gradient iteration algorithm can be
  easily and efficiently implemented in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: belkin16
month: 0
firstpage: 446
lastpage: 487
page: 446-487
sections: 
author:
- given: Mikhail
  family: Belkin
- given: Luis
  family: Rademacher
- given: James
  family: Voss
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/belkin16/belkin16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
