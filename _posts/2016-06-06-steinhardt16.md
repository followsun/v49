---
title: Memory, Communication, and Statistical Queries
abstract: 'If a concept class can be represented with a certain amount of memory,
  can it be efficiently learned with the same amount of memory? What concepts can
  be efficiently learned by algorithms that extract only a few bits of information
  from each example? We introduce a formal framework for studying these questions,
  and investigate the relationship between the fundamental resources of memory or
  communication and the sample complexity of the learning task. We relate our memory-bounded
  and communication-bounded learning models to the well-studied statistical query
  model. This connection can be leveraged to obtain both upper and lower bounds: we
  show strong lower bounds on learning parity functions with bounded communication,
  as well as upper bounds on solving sparse linear regression problems with limited
  memory.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: steinhardt16
month: 0
firstpage: 1490
lastpage: 1516
page: 1490-1516
sections: 
author:
- given: Jacob
  family: Steinhardt
- given: Gregory
  family: Valiant
- given: Stefan
  family: Wager
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/steinhardt16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
