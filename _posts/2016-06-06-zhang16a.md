---
title: The Extended Littlestone’s Dimension for Learning with Mistakes and Abstentions
abstract: This paper studies classification with an abstention option in the online
  setting. In this setting, examples arrive sequentially, the learner is given a hypothesis
  class \mathcalH, and the goal of the learner is to either predict a label on each
  example or abstain, while ensuring that it does not make more than a pre-specified
  number of mistakes when it does predict a label. Previous work on this problem has
  left open two main challenges. First, not much is known about the optimality of
  algorithms, and in particular, about what an optimal algorithmic strategy is for
  any individual hypothesis class. Second, while the realizable case has been studied,
  the more realistic non-realizable scenario is not well-understood. In this paper,
  we address both challenges. First, we provide a novel measure, called the Extended
  Littlestone’s Dimension, which captures the number of abstentions needed to ensure
  a certain number of mistakes. Second, we explore the non-realizable case, and provide
  upper and lower bounds on the number of abstentions required by an algorithm to
  guarantee a specified number of mistakes.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang16a
month: 0
tex_title: The Extended Littlestone's Dimension for Learning with Mistakes and Abstentions
firstpage: 1584
lastpage: 1616
page: 1584-1616
sections: 
author:
- given: Chicheng
  family: Zhang
- given: Kamalika
  family: Chaudhuri
date: 2016-06-06
address: Columbia University, New York, New York, USA
publisher: PMLR
container-title: 29th Annual Conference on Learning Theory
volume: '49'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 6
  - 6
pdf: http://proceedings.mlr.press/v49/zhang16a.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
