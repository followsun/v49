<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Asymptotic behavior of <span class="math">\(\ell_p\)</span>-based <span>L</span>aplacian regularization in semi-supervised learning | COLT 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Asymptotic behavior of $\ell_p$-based {L}aplacian regularization in semi-supervised learning">

  <meta name="citation_author" content="&lt;span&gt;El Alaoui&lt;/span&gt;, Ahmed">

  <meta name="citation_author" content="Cheng, Xiang">

  <meta name="citation_author" content="Ramdas, Aaditya">

  <meta name="citation_author" content="Wainwright, Martin J.">

  <meta name="citation_author" content="Jordan, Michael I.">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="29th Annual Conference on Learning Theory">
<meta name="citation_firstpage" content="879">
<meta name="citation_lastpage" content="906">
<meta name="citation_pdf_url" content="http://jmlr.org/proceedings/papers/v49/elalaoui16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Asymptotic behavior of <span class="math">\(\ell_p\)</span>-based <span>L</span>aplacian regularization in semi-supervised learning</h1>

	<div id="authors">
	
		Ahmed <span>El Alaoui</span>,
	
		Xiang Cheng,
	
		Aaditya Ramdas,
	
		Martin J. Wainwright,
	
		Michael I. Jordan
	<br />
	</div>
	<div id="info">
		29th Annual Conference on Learning Theory,
		pp. 879–906, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Given a weighted graph with <span class="math">\(N\)</span> vertices, consider a real-valued regression problem in a semi-supervised setting, where one observes <span class="math">\(n\)</span> labeled vertices, and the task is to label the remaining ones. We present a theoretical study of <span class="math">\(\ell_p\)</span>-based Laplacian regularization under a <span class="math">\(d\)</span>-dimensional geometric random graph model. We provide a variational characterization of the performance of this regularized learner as <span class="math">\(N\)</span> grows to infinity while <span class="math">\(n\)</span> stays constant; the associated optimality conditions lead to a partial differential equation that must be satisfied by the associated function estimate <span class="math">\(\widehat{f}\)</span>. From this formulation we derive several predictions on the limiting behavior the function <span class="math">\(\fhat\)</span>, including (a) a phase transition in its smoothness at the threshold <span class="math">\(p = d + 1\)</span>; and (b) a tradeoff between smoothness and sensitivity to the underlying unlabeled data distribution <span class="math">\(P\)</span>. Thus, over the range <span class="math">\(p \leq d\)</span>, the function estimate <span class="math">\(\widehat{f}\)</span> is degenerate and “spiky,” whereas for <span class="math">\(p\geq d+1\)</span>, the function estimate <span class="math">\(\fhat\)</span> is smooth. We show that the effect of the underlying density vanishes monotonically with <span class="math">\(p\)</span>, such that in the limit <span class="math">\(p = \infty\)</span>, corresponding to the so-called Absolutely Minimal Lipschitz Extension, the estimate <span class="math">\(\widehat{f}\)</span> is independent of the distribution <span class="math">\(P\)</span>. Under the assumption of semi-supervised smoothness, ignoring <span class="math">\(P\)</span> can lead to poor statistical performance; in particular, we construct a specific example for <span class="math">\(d=1\)</span> to demonstrate that <span class="math">\(p=2\)</span> has lower risk than <span class="math">\(p=\infty\)</span> due to the former penalty adapting to <span class="math">\(P\)</span> and the latter ignoring it. We also provide simulations that verify the accuracy of our predictions for finite sample sizes. Together, these properties show that <span class="math">\(p = d+1\)</span> is an optimal choice, yielding a function estimate <span class="math">\(\fhat\)</span> that is both smooth and non-degenerate, while remaining maximally sensitive to <span class="math">\(P\)</span>.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="elalaoui16.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
